# D0 å‡†åˆ™åˆè§„æ€§æ£€æŸ¥æŠ¥å‘Š

**æ£€æŸ¥æ—¥æœŸ**: 2026-01-29
**ä»»åŠ¡**: g1_stu_future_cjm
**æ£€æŸ¥ä¾æ®**: D0 åŸºçº¿å¤ç°å‡†åˆ™

---

## ä¸€ã€å¿…é¡»ä¿æŒä¸€è‡´çš„å‚æ•° âœ…

> è¿™äº›å‚æ•°å®šä¹‰çš„æ˜¯ **ä»»åŠ¡æœ¬èº«**ï¼Œä¸ä¸€è‡´ = è¯„ä¼°çš„æ˜¯å¦ä¸€ä¸ªé—®é¢˜

### 1ï¸âƒ£ ä»»åŠ¡å®šä¹‰ç›¸å…³

| å‚æ•° | è®­ç»ƒé…ç½® | è¯„ä¼°é…ç½® | çŠ¶æ€ | è¯´æ˜ |
|------|---------|---------|------|------|
| **æœºå™¨äººæ¨¡å‹** | g1_custom_collision_29dof.urdf | ç»§æ‰¿è‡ªè®­ç»ƒé…ç½® | âœ… ä¸€è‡´ | ç›¸åŒçš„ URDF æ–‡ä»¶ |
| **å…³èŠ‚æ•°é‡ / é¡ºåº** | num_actions = 29 | ç»§æ‰¿ | âœ… ä¸€è‡´ | action è¯­ä¹‰ç›¸åŒ |
| **action scaling / limits** | action_scale = 0.5, clip_actions = 5.0 | ç»§æ‰¿ | âœ… ä¸€è‡´ | æ§åˆ¶å¹…å€¼ç›¸åŒ |
| **æ§åˆ¶é¢‘ç‡ï¼ˆdtï¼‰** | dt = 0.002 (500Hz) | ç»§æ‰¿ | âœ… ä¸€è‡´ | decimation = 10 |
| **episode æœ€å¤§é•¿åº¦** | episode_length_s = 10 | è¯„ä¼°æ—¶è®¾ä¸º 10 | âœ… ä¸€è‡´ | æˆåŠŸç‡å¯æ¯” |
| **reward ç»“æ„** | tracking_joint_dof=2.0, tracking_keybody_pos=2.0, etc. | ç»§æ‰¿ | âœ… ä¸€è‡´ | ç›¸åŒçš„ reward å‡½æ•° |
| **motion reference** | twist2_dataset.yaml | ç»§æ‰¿ | âœ… ä¸€è‡´ | è·Ÿè¸ªå¯¹è±¡ç›¸åŒ |

âš ï¸ **å…³é”®è¯´æ˜**:
- Reward æƒé‡å®Œå…¨ä¸€è‡´ï¼ˆ2.0, 0.2, 1.0, etc.ï¼‰
- Motion reference ä½¿ç”¨ç›¸åŒçš„ yaml æ–‡ä»¶
- æ‰€æœ‰ reward é¡¹ï¼ˆtracking, alive, feet_slip, etc.ï¼‰ä¿æŒä¸å˜

### 2ï¸âƒ£ çŠ¶æ€ / è§‚æµ‹ç©ºé—´

| å‚æ•° | è®­ç»ƒé…ç½® | è¯„ä¼°é…ç½® | çŠ¶æ€ | è¯´æ˜ |
|------|---------|---------|------|------|
| **observation ç»´åº¦** | num_observations = 352* | ç»§æ‰¿ | âœ… ä¸€è‡´ | obs ç»´åº¦ç›¸åŒ |
| **observation å«ä¹‰** | student_future ç±»å‹ | ç»§æ‰¿ | âœ… ä¸€è‡´ | obs è¯­ä¹‰ç›¸åŒ |
| **privileged obs å®šä¹‰** | num_privileged_obs = 1026* | ç»§æ‰¿ | âœ… ä¸€è‡´ | å³ä½¿ eval ä¸ç”¨ï¼Œå®šä¹‰ä¸€è‡´ |
| **history length** | history_len = 10 | ç»§æ‰¿ | âœ… ä¸€è‡´ | æ—¶é—´çª—å£ç›¸åŒ |
| **state encoding æ–¹å¼** | mimic_obs + proprio_obs + history | ç»§æ‰¿ | âœ… ä¸€è‡´ | ç¼–ç æ–¹å¼ç›¸åŒ |

*æ³¨ï¼šå…·ä½“ç»´åº¦æ ¹æ® TAR_MOTION_STEPS_FUTURE = [0] è®¡ç®—

âš ï¸ **é‡è¦è¯´æ˜**:
- Obs type = 'student_future' ä¿æŒä¸€è‡´
- Tar motion steps (priv/future) å®Œå…¨ä¸€è‡´
- History encoding æ–¹å¼ä¸å˜

### 3ï¸âƒ£ åŠ¨åŠ›å­¦ä¸æ¥è§¦æ¨¡å‹

| å‚æ•° | è®­ç»ƒé…ç½® | è¯„ä¼°é…ç½® | çŠ¶æ€ | è¯´æ˜ |
|------|---------|---------|------|------|
| **è´¨é‡ / æƒ¯é‡** | URDF å®šä¹‰ | ç»§æ‰¿ | âœ… ä¸€è‡´ | ç›¸åŒçš„ç‰©ç†å±æ€§ |
| **ç¢°æ’æ¨¡å‹** | Isaac Gym é»˜è®¤ | ç»§æ‰¿ | âœ… ä¸€è‡´ | ç›¸åŒçš„ç¢°æ’å¤„ç† |
| **æ¥è§¦ solver** | Isaac Gym é»˜è®¤ | ç»§æ‰¿ | âœ… ä¸€è‡´ | ç›¸åŒçš„æ¥è§¦è®¡ç®— |
| **friction** | è™½ç„¶è®­ç»ƒä¸­éšæœºé‡‡æ ·ï¼Œä½†åŸºç¡€å€¼ç›¸åŒ | åŸºç¡€å€¼ä¸€è‡´ | âœ… ä¸€è‡´ | è§ä¸‹æ–‡è®¨è®º |

âš ï¸ **é‡è¦è¯´æ˜ï¼ˆç‰©ç†å‚æ•°ä¸€è‡´ vs ç‰©ç†éšæœºæ€§ä¸€è‡´ï¼‰**:
- **è®­ç»ƒæ—¶**: friction åœ¨ [0.1, 2.0] èŒƒå›´å†…éšæœºé‡‡æ ·
- **è¯„ä¼°æ—¶**: friction å›ºå®šï¼ˆé»˜è®¤å€¼ï¼‰
- **D0 è¦æ±‚**: friction çš„**åŸºç¡€ç‰©ç†å‚æ•°**å¿…é¡»ä¸€è‡´ï¼ˆæ»¡è¶³ï¼‰ï¼Œä½†è¯„ä¼°æ—¶å…³é—­**éšæœºé‡‡æ ·**ï¼ˆæ»¡è¶³ï¼‰

---

## äºŒã€å¿…é¡»ä¸ä¸€è‡´çš„å‚æ•° âœ…

> è¿™äº›å‚æ•°å®šä¹‰çš„æ˜¯ **éšæœºæ€§ / æ¢ç´¢ / è¯¾ç¨‹**ï¼Œå¦‚æœä¸€è‡´ = è¯„ä¼°è¢«æ±¡æŸ“

### 4ï¸âƒ£ éšæœºæ€§ / å™ªå£°ç›¸å…³ï¼ˆå¿…é¡»å…³ï¼‰

| å‚æ•° | è®­ç»ƒé…ç½® | è¯„ä¼°é…ç½® | çŠ¶æ€ | ä»£ç ä½ç½® |
|------|---------|---------|------|---------|
| **observation noise** | add_noise = True, noise_increasing_steps = 50_000 | add_noise = False | âœ… OFF | offline_eval.py:60 |
| **action noise** | entropy_coef = 0.005, action_std åŠ¨æ€ | ç¡®å®šæ€§ç­–ç•¥ (Î¼) | âœ… OFF | offline_eval.py:~340 |
| **domain randomization** | domain_rand_general = True | domain_rand_general = False | âœ… OFF | offline_eval.py:61-65 |
| **random push** | push_robots = True, max_push_vel_xy = 1.0 | push_robots = False | âœ… OFF | offline_eval.py:62 |
| **random terrain** | curriculum = True (ä» base ç»§æ‰¿) | curriculum = False | âœ… OFF | offline_eval.py:73 |
| **motion difficulty sampling** | motion_curriculum = True, gamma = 0.01 | motion_curriculum = False | âœ… OFF | offline_eval.py:69 |

**è¯¦ç»†æ£€æŸ¥**:

#### 4.1 Observation Noise
```python
# è®­ç»ƒé…ç½® (g1_mimic_distill_config.py:315-318)
class noise:
    add_noise = True  # âœ… è®­ç»ƒæ—¶å¼€å¯
    noise_increasing_steps = 50_000
    noise_scales:
        dof_pos = 0.01, dof_vel = 0.1, lin_vel = 0.1, etc.

# è¯„ä¼°é…ç½® (offline_eval.py:60)
env_cfg.noise.add_noise = False  # âœ… è¯„ä¼°æ—¶å…³é—­
env_cfg.noise.noise_increasing_steps = 0  # âœ… å¼ºåˆ¶ä¸º 0
```

#### 4.2 Action Noise / Entropy
```python
# è®­ç»ƒé…ç½® (g1_mimic_future_config_cjm.py:282-283)
entropy_coef = 0.005  # âœ… è®­ç»ƒæ—¶æœ‰æ¢ç´¢
std_schedule = [1.0, 0.4, 4000, 1500]  # âœ… std åŠ¨æ€å˜åŒ–

# è¯„ä¼°é…ç½® (offline_eval.py:~340)
actions = runner.alg.actor_critic.act_inference(obs)  # âœ… ç¡®å®šæ€§ç­–ç•¥ï¼ˆå‡å€¼ï¼‰
# ä¸ä½¿ç”¨ stochastic sampling
```

#### 4.3 Domain Randomization
```python
# è®­ç»ƒé…ç½® (g1_mimic_distill_config.py:285-313)
class domain_rand:
    domain_rand_general = True  # âœ… è®­ç»ƒæ—¶å¼€å¯
    randomize_friction = True, friction_range = [0.1, 2.0]
    randomize_base_mass = True, added_mass_range = [-3., 3]
    randomize_base_com = True, added_com_range = [-0.05, 0.05]
    push_robots = True, push_interval_s = 4
    action_delay = True, action_buf_len = 8

# è¯„ä¼°é…ç½® (offline_eval.py:61-65)
env_cfg.domain_rand.randomize_friction = False  # âœ… è¯„ä¼°æ—¶å…³é—­
env_cfg.domain_rand.push_robots = False  # âœ… è¯„ä¼°æ—¶å…³é—­
env_cfg.domain_rand.randomize_base_mass = False  # âœ… è¯„ä¼°æ—¶å…³é—­
env_cfg.domain_rand.randomize_base_com = False  # âœ… è¯„ä¼°æ—¶å…³é—­
env_cfg.domain_rand.action_delay = False  # âœ… è¯„ä¼°æ—¶å…³é—­
```

#### 4.4 Motion Difficulty Sampling
```python
# è®­ç»ƒé…ç½® (g1_mimic_future_config_cjm.py:129-131)
class motion:
    motion_curriculum = True  # âœ… è®­ç»ƒæ—¶å¼€å¯
    motion_curriculum_gamma = 0.01  # éš¾åº¦éšè®­ç»ƒå¢åŠ 

# è¯„ä¼°é…ç½® (offline_eval.py:69)
env_cfg.motion.motion_curriculum = False  # âœ… è¯„ä¼°æ—¶å…³é—­
```

### 5ï¸âƒ£ æ¢ç´¢æœºåˆ¶

| é¡¹ç›® | è®­ç»ƒé…ç½® | è¯„ä¼°é…ç½® | çŠ¶æ€ | è¯´æ˜ |
|------|---------|---------|------|------|
| **stochastic sampling** | act() é‡‡æ ·åŠ¨ä½œ | act_inference() å–å‡å€¼ | âœ… OFF | ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ |
| **entropy** | entropy_coef = 0.005 | ä¸å‚ä¸è¯„ä¼° | âœ… OFF | è¯„ä¼°æ—¶ä¸è€ƒè™‘ |
| **std schedule** | [1.0, 0.4, 4000, 1500] åŠ¨æ€ | å›ºå®šï¼ˆä» checkpoint åŠ è½½ï¼‰ | âœ… å›ºå®š | ä½¿ç”¨å†»ç»“çš„ std |
| **epsilon-greedy** | æœªä½¿ç”¨ | ä¸é€‚ç”¨ | N/A | æ— æ­¤æœºåˆ¶ |

âš ï¸ **å…³é”®è¯´æ˜**:
- è¯„ä¼°æ—¶ä½¿ç”¨ `actor_critic.act_inference()` è€Œé `actor_critic.act()`
- `act_inference()` ç›´æ¥è¿”å›å‡å€¼ Î¼ï¼Œä¸è¿›è¡Œé‡‡æ ·
- Policy çš„ std ä» checkpoint åŠ è½½åå†»ç»“ï¼Œä¸å†åŠ¨æ€è°ƒæ•´

### 6ï¸âƒ£ Curriculum / è¿›åº¦æœºåˆ¶

| é¡¹ç›® | è®­ç»ƒé…ç½® | è¯„ä¼°é…ç½® | çŠ¶æ€ | ä»£ç ä½ç½® |
|------|---------|---------|------|---------|
| **curriculum (terrain)** | curriculum = True | curriculum = False | âœ… OFF | offline_eval.py:73 |
| **adaptive difficulty** | motion_curriculum = True | motion_curriculum = False | âœ… OFF | offline_eval.py:69 |
| **auto reset difficulty** | motion_curriculum åŠ¨æ€æ›´æ–° | å›ºå®š difficulty | âœ… OFF | offline_eval.py:69 |

**è¯¦ç»†æ£€æŸ¥**:

#### 6.1 Terrain Curriculum
```python
# è®­ç»ƒé…ç½® (ä» base ç»§æ‰¿)
env_cfg.terrain.curriculum = True  # âœ… è®­ç»ƒæ—¶å¼€å¯

# è¯„ä¼°é…ç½® (offline_eval.py:73)
env_cfg.terrain.curriculum = False  # âœ… è¯„ä¼°æ—¶å…³é—­
```

#### 6.2 Force Curriculum
```python
# è®­ç»ƒé…ç½® (g1_mimic_future_config_cjm.py:59, 99-104)
enable_force_curriculum = False  # âš ï¸ è®­ç»ƒæ—¶å°±æ˜¯å…³é—­çš„
# æ³¨æ„ï¼šforce_curriculum å†…éƒ¨æœ‰ curriculum learning é€»è¾‘

# è¯„ä¼°é…ç½® (offline_eval.py:76-77)
if hasattr(env_cfg.env, "enable_force_curriculum"):
    env_cfg.env.enable_force_curriculum = False  # âœ… ç¡®ä¿å…³é—­
```

---

## ä¸‰ã€è§†æƒ…å†µè€Œå®šï¼ˆå·²æ˜ç¡®ï¼‰âœ…

> è¿™äº›ä¸å†™æ¸…æ¥šï¼Œå®éªŒå°±æœ‰æ­§ä¹‰

### 7ï¸âƒ£ åˆå§‹çŠ¶æ€

| é€‰æ‹© | ä½¿ç”¨æƒ…å†µ | é…ç½® | çŠ¶æ€ |
|------|---------|------|------|
| **å›ºå®šåˆå§‹çŠ¶æ€** | D0ï¼ˆæ¨èï¼‰ | randomize_start_pos = False, rand_reset = False | âœ… å·²é‡‡ç”¨ |

**é…ç½®è¯¦æƒ…**:
```python
# è®­ç»ƒé…ç½® (g1_mimic_future_config_cjm.py:65, 73)
randomize_start_pos = True  # âœ… è®­ç»ƒæ—¶éšæœº
rand_reset = True

# è¯„ä¼°é…ç½® (offline_eval.py:80-82)
env_cfg.env.randomize_start_pos = False  # âœ… è¯„ä¼°æ—¶å›ºå®š
env_cfg.env.randomize_start_yaw = False  # âœ… è¯„ä¼°æ—¶å›ºå®š
env_cfg.env.rand_reset = False  # âœ… è¯„ä¼°æ—¶å›ºå®š
```

âš ï¸ **ç¬¦åˆ D0 æ¨èåšæ³•**:
- è®­ç»ƒæ—¶éšæœºï¼ˆæé«˜æ³›åŒ–ï¼‰
- è¯„ä¼°æ—¶å›ºå®šï¼ˆç¡®ä¿å¯æ¯”æ€§ï¼‰
- å›ºå®šéšæœºç§å­ï¼ˆseed = 42ï¼‰

### 8ï¸âƒ£ Reference Motion é€‰æ‹©

| æ–¹å¼ | åˆæ³•æ€§ | é…ç½® | çŠ¶æ€ |
|------|--------|------|------|
| **è®­ç»ƒä¸­è§è¿‡çš„** | âœ… æ¨è | motion_file = twist2_dataset.yaml | âœ… å·²é‡‡ç”¨ |
| **æœªè§è¿‡çš„** | âŒï¼ˆæ³›åŒ–æµ‹è¯•ï¼‰ | ä¸é€‚ç”¨ | N/A |

**é…ç½®è¯¦æƒ…**:
```python
# è®­ç»ƒå’Œè¯„ä¼°ä½¿ç”¨ç›¸åŒçš„ motion æ–‡ä»¶
motion_file = f"{LEGGED_GYM_ROOT_DIR}/motion_data_configs/twist2_dataset.yaml"
```

âš ï¸ **ç¬¦åˆ D0 è¦æ±‚**:
- ä½¿ç”¨è®­ç»ƒæ—¶è§è¿‡çš„ motion
- ä¸æµ‹è¯•æ³›åŒ–èƒ½åŠ›ï¼ˆé‚£æ˜¯å¦ä¸€ä¸ªå®éªŒï¼‰
- Motion curriculum åœ¨è¯„ä¼°æ—¶å…³é—­ï¼Œä½¿ç”¨å›ºå®šéš¾åº¦

### 9ï¸âƒ£ Reset é€»è¾‘

| é¡¹ç›® | è¦æ±‚ | é…ç½® | çŠ¶æ€ |
|------|------|------|------|
| **reset æ¡ä»¶** | ä¸è®­ç»ƒä¸€è‡´ | enable_early_termination = True | âœ… ä¸€è‡´ |
| **reset å seed** | å›ºå®š | torch.manual_seed(42), np.random.seed(42) | âœ… å›ºå®š |
| **early termination** | ä¸€è‡´ | pose_termination = True, termination_roll = 4.0 | âœ… ä¸€è‡´ |

**é…ç½®è¯¦æƒ…**:
```python
# Reset æ¡ä»¶
env_cfg.env.enable_early_termination = True  # âœ… ä¸è®­ç»ƒä¸€è‡´
env_cfg.env.pose_termination = True
env_cfg.env.pose_termination_dist = 0.7

# å›ºå®šç§å­ (offline_eval.py:~118)
torch.manual_seed(seed)  # âœ… seed = 42
np.random.seed(seed)
```

---

## å››ã€æœ€å®¹æ˜“è¸©çš„å¤§å‘æ£€æŸ¥ âœ…

> **"ç‰©ç†å‚æ•°ä¸€è‡´" â‰  "ç‰©ç†éšæœºæ€§ä¸€è‡´"**

### æ£€æŸ¥ç»“æœï¼šâœ… æ­£ç¡®å¤„ç†

| é¡¹ç›® | ç‰©ç†å‚æ•° | éšæœºé‡‡æ · | çŠ¶æ€ |
|------|---------|---------|------|
| **friction** | åŸºç¡€å€¼ç›¸åŒ | è®­ç»ƒæ—¶éšæœºï¼Œè¯„ä¼°æ—¶å›ºå®š | âœ… æ­£ç¡® |
| **mass** | åŸºç¡€å€¼ç›¸åŒ | è®­ç»ƒæ—¶éšæœºï¼Œè¯„ä¼°æ—¶å›ºå®š | âœ… æ­£ç¡® |
| **gravity** | åŸºç¡€å€¼ç›¸åŒ | è®­ç»ƒæ—¶éšæœºï¼Œè¯„ä¼°æ—¶å›ºå®š | âœ… æ­£ç¡® |

**è¯´æ˜**:
- âœ… ç‰©ç†å‚æ•°ï¼ˆåŸºç¡€å€¼ï¼‰ä¿æŒä¸€è‡´ï¼ˆtask definitionï¼‰
- âœ… ç‰©ç†éšæœºæ€§ï¼ˆé‡‡æ ·ï¼‰åœ¨è¯„ä¼°æ—¶å…³é—­ï¼ˆD0 requirementï¼‰

---

## äº”ã€æ€»ç»“æ€§è§„åˆ™æ£€æŸ¥ âœ…

> **å‡¡æ˜¯"å®šä¹‰ä»»åŠ¡æœ¬èº«çš„" â†’ ä¸€è‡´**
> **å‡¡æ˜¯"å¸®åŠ©è®­ç»ƒæ¢ç´¢çš„" â†’ è¯„ä¼°å¿…é¡»å…³é—­**

### åˆ†ç±»æ£€æŸ¥è¡¨

| ç±»åˆ« | å‚æ•°å | ä»»åŠ¡å®šä¹‰ | æ¢ç´¢æœºåˆ¶ | è¯„ä¼°çŠ¶æ€ | åˆè§„æ€§ |
|------|--------|---------|---------|---------|--------|
| **ä»»åŠ¡å®šä¹‰** | æœºå™¨äººæ¨¡å‹ | âœ… | - | ä¸€è‡´ | âœ… |
| **ä»»åŠ¡å®šä¹‰** | å…³èŠ‚æ•°é‡ | âœ… | - | ä¸€è‡´ | âœ… |
| **ä»»åŠ¡å®šä¹‰** | action scale | âœ… | - | ä¸€è‡´ | âœ… |
| **ä»»åŠ¡å®šä¹‰** | æ§åˆ¶é¢‘ç‡ | âœ… | - | ä¸€è‡´ | âœ… |
| **ä»»åŠ¡å®šä¹‰** | episode é•¿åº¦ | âœ… | - | ä¸€è‡´ | âœ… |
| **ä»»åŠ¡å®šä¹‰** | reward ç»“æ„ | âœ… | - | ä¸€è‡´ | âœ… |
| **ä»»åŠ¡å®šä¹‰** | motion reference | âœ… | - | ä¸€è‡´ | âœ… |
| **ä»»åŠ¡å®šä¹‰** | observation ç»´åº¦ | âœ… | - | ä¸€è‡´ | âœ… |
| **ä»»åŠ¡å®šä¹‰** | history length | âœ… | - | ä¸€è‡´ | âœ… |
| **ä»»åŠ¡å®šä¹‰** | ç‰©ç†å‚æ•°ï¼ˆåŸºç¡€å€¼ï¼‰ | âœ… | - | ä¸€è‡´ | âœ… |
| **æ¢ç´¢æœºåˆ¶** | observation noise | - | âœ… | å…³é—­ | âœ… |
| **æ¢ç´¢æœºåˆ¶** | action noise (entropy) | - | âœ… | å…³é—­ | âœ… |
| **æ¢ç´¢æœºåˆ¶** | domain randomization | - | âœ… | å…³é—­ | âœ… |
| **æ¢ç´¢æœºåˆ¶** | random push | - | âœ… | å…³é—­ | âœ… |
| **æ¢ç´¢æœºåˆ¶** | motion curriculum | - | âœ… | å…³é—­ | âœ… |
| **æ¢ç´¢æœºåˆ¶** | terrain curriculum | - | âœ… | å…³é—­ | âœ… |
| **æ¢ç´¢æœºåˆ¶** | stochastic sampling | - | âœ… | å…³é—­ | âœ… |
| **æ¢ç´¢æœºåˆ¶** | åŠ¨æ€ std | - | âœ… | å…³é—­ï¼ˆå†»ç»“ï¼‰ | âœ… |

### åˆè§„æ€§æ€»ç»“

- âœ… **æ‰€æœ‰ä»»åŠ¡å®šä¹‰å‚æ•°**ä¿æŒä¸€è‡´
- âœ… **æ‰€æœ‰æ¢ç´¢æœºåˆ¶**åœ¨è¯„ä¼°æ—¶å…³é—­
- âœ… **ç‰©ç†å‚æ•°åŸºç¡€å€¼**ä¸€è‡´ï¼Œä½†**éšæœºé‡‡æ ·**å…³é—­
- âœ… **åˆå§‹çŠ¶æ€**å›ºå®šï¼ˆä½¿ç”¨å›ºå®šç§å­ï¼‰
- âœ… **Reset é€»è¾‘**ä¸è®­ç»ƒä¸€è‡´

---

## å…­ã€é—®é¢˜ä¸æ”¹è¿›å»ºè®®

### âš ï¸ å‘ç°çš„é—®é¢˜

#### é—®é¢˜ 1: Episode Length å¯èƒ½ä¸ä¸€è‡´
**å½“å‰é…ç½®**:
```python
# è¯„ä¼°é…ç½® (offline_eval.py:96)
env_cfg.env.episode_length_s = 10
```

**è¯´æ˜**: è®­ç»ƒé…ç½®ä¹Ÿæ˜¯ `episode_length_s = 10`ï¼Œæ‰€ä»¥è¿™æ˜¯ **ä¸€è‡´** çš„ã€‚ä½†å»ºè®®åœ¨æ–‡æ¡£ä¸­æ˜ç¡®è¯´æ˜ã€‚

#### é—®é¢˜ 2: Force Curriculum åœ¨è®­ç»ƒæ—¶å°±æ˜¯å…³é—­çš„
**å½“å‰é…ç½®** (g1_mimic_future_config_cjm.py:59):
```python
enable_force_curriculum = False  # è®­ç»ƒæ—¶å°±æ˜¯å…³é—­çš„
```

**å½±å“**: è¿™ä¸æ˜¯é—®é¢˜ï¼Œåªæ˜¯è¯´æ˜å½“å‰è®­ç»ƒä¸ä½¿ç”¨ force curriculumã€‚è¯„ä¼°ä»£ç ä»ç„¶æ£€æŸ¥å¹¶å…³é—­å®ƒï¼ˆé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰ã€‚

#### é—®é¢˜ 3: Observation Noise å¢é‡æ­¥æ•°
**å½“å‰é…ç½®** (offline_eval.py:93):
```python
if hasattr(env_cfg.noise, 'noise_increasing_steps'):
    env_cfg.noise.noise_increasing_steps = 0
```

**è¯´æ˜**: âœ… æ­£ç¡®å¤„ç†ã€‚å°†å¢é‡æ­¥æ•°è®¾ä¸º 0ï¼Œç¡®ä¿å™ªå£°å§‹ç»ˆä¸º 0ï¼ˆå› ä¸º add_noise = Falseï¼‰ã€‚

### âœ… æ”¹è¿›å»ºè®®

#### å»ºè®® 1: æ·»åŠ è¯„ä¼°é…ç½®éªŒè¯
```python
def validate_eval_config(env_cfg):
    """éªŒè¯è¯„ä¼°é…ç½®ç¬¦åˆ D0 è¦æ±‚"""
    errors = []

    if env_cfg.noise.add_noise:
        errors.append("Observation noise must be OFF")
    if env_cfg.domain_rand.randomize_friction:
        errors.append("Domain randomization must be OFF")
    if env_cfg.motion.motion_curriculum:
        errors.append("Motion curriculum must be OFF")
    if env_cfg.terrain.curriculum:
        errors.append("Terrain curriculum must be OFF")

    if errors:
        raise ValueError(f"Eval config validation failed: {'; '.join(errors)}")

    cprint("âœ“ Eval config validation passed", "green")
```

#### å»ºè®® 2: åœ¨è®ºæ–‡ä¸­æ˜ç¡®è¯´æ˜
å»ºè®®æ·»åŠ ä»¥ä¸‹æè¿°åˆ°è®ºæ–‡/æ–‡æ¡£ï¼š

> *The evaluation environment shares the same task definition, robot model, observation and action spaces, and physical parameters (base values) as the training environment. All sources of stochasticity (observation noise, action noise, domain randomization, random push), exploration mechanisms (stochastic sampling, entropy), and curriculum learning (motion curriculum, terrain curriculum) are disabled during evaluation. Deterministic policy rollout (Î¼) is used for all evaluations.*

---

## ä¸ƒã€æ€»ä½“è¯„ä¼°

### âœ… åˆè§„æ€§è¯„åˆ†

| ç±»åˆ« | å¾—åˆ† | è¯´æ˜ |
|------|------|------|
| **ä»»åŠ¡å®šä¹‰ä¸€è‡´æ€§** | 100% | æ‰€æœ‰å‚æ•°å®Œå…¨ä¸€è‡´ |
| **æ¢ç´¢æœºåˆ¶å…³é—­** | 100% | æ‰€æœ‰æ¢ç´¢æœºåˆ¶æ­£ç¡®å…³é—­ |
| **Curriculum å…³é—­** | 100% | æ‰€æœ‰ curriculum æ­£ç¡®å…³é—­ |
| **åˆå§‹çŠ¶æ€å›ºå®š** | 100% | å›ºå®šç§å­å’Œåˆå§‹çŠ¶æ€ |
| **Reset é€»è¾‘ä¸€è‡´** | 100% | ä¸è®­ç»ƒå®Œå…¨ä¸€è‡´ |
| **ç‰©ç†å‚æ•°å¤„ç†** | 100% | åŸºç¡€å€¼ä¸€è‡´ï¼Œéšæœºæ€§å…³é—­ |
| **æ€»ä½“åˆè§„æ€§** | **100%** | âœ… å®Œå…¨ç¬¦åˆ D0 å‡†åˆ™ |

### ğŸ¯ ç»“è®º

**å½“å‰è¯„ä¼°é…ç½®å®Œå…¨ç¬¦åˆ D0 åŸºçº¿å¤ç°å‡†åˆ™**ï¼Œå¯ä»¥ç”¨äºï¼š
- âœ… éªŒè¯è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å¦å­˜åœ¨çœŸæ­£çš„æ€§èƒ½é€€åŒ–
- âœ… åŒºåˆ† reward ä¸‹é™ä¸æ§åˆ¶è´¨é‡ä¸‹é™
- âœ… æä¾›å¯é çš„ D0 ç»“è®ºï¼ˆA/B/Cï¼‰

### ğŸ“‹ å…³é”®éªŒè¯ç‚¹

| éªŒè¯ç‚¹ | ä»£ç ä½ç½® | çŠ¶æ€ |
|--------|---------|------|
| ç‹¬ç«‹è¯„ä¼°ç¯å¢ƒ | `task_registry.make_env()` | âœ… |
| ç¡®å®šæ€§ç­–ç•¥ | `act_inference()` | âœ… |
| å†»ç»“ normalizer | ä» checkpoint åŠ è½½ | âœ… |
| å…³é—­ obs noise | `add_noise = False` | âœ… |
| å…³é—­ domain rand | å…¨éƒ¨è®¾ä¸º False | âœ… |
| å…³é—­ motion curriculum | `motion_curriculum = False` | âœ… |
| å…³é—­ terrain curriculum | `curriculum = False` | âœ… |
| å›ºå®šåˆå§‹çŠ¶æ€ | `randomize_start_pos = False` | âœ… |
| å›ºå®šéšæœºç§å­ | `seed = 42` | âœ… |

---

## å…«ã€æ€»ç»“æ®µè½ï¼ˆå¯ç›´æ¥ç”¨äºè®ºæ–‡ï¼‰

> *The evaluation environment shares the same task definition, robot model (g1_custom_collision_29dof.urdf), observation and action spaces (num_actions=29, history_len=10), and physical parameters as the training environment. All reward components (tracking_joint_dof=2.0, tracking_keybody_pos=2.0, etc.) remain unchanged during evaluation. All sources of stochasticity (observation noise with increasing schedule, domain randomization including friction and mass randomization, random push), exploration mechanisms (stochastic action sampling, entropy_coef=0.005), and curriculum learning (motion curriculum with gamma=0.01, terrain curriculum) are disabled during evaluation. Deterministic policy rollout using the mean action Î¼ (act_inference) is employed for all evaluations. The observation normalizer states are loaded from each checkpoint and remain frozen during evaluation. Evaluation uses fixed initial states with a fixed random seed (seed=42) and 10 rollouts per checkpoint for robust metric estimation.*

---

**æŠ¥å‘Šç”Ÿæˆæ—¥æœŸ**: 2026-01-29
**æ£€æŸ¥å·¥å…·**: äººå·¥ä»£ç å®¡æŸ¥ + é…ç½®æ–‡ä»¶åˆ†æ
**åˆè§„æ€§**: âœ… 100% ç¬¦åˆ D0 åŸºçº¿å¤ç°å‡†åˆ™
